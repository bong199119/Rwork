round(apply(data,1,mean),2)
round(apply(data,2,mean),2)
score <- c(90,85,83)
# 분산
var(score)
# 분산
var(score) # 13
# 표준편차 : 분산의 양의 제곱근
sd(score)
# 표준편차 : 분산의 양의 제곱근
sd(score) # 3.605551
sqrt(var(score))
sqrt(var(score)) # 3.605551
#분산 = sum((x-산술평균)^2) / n-1
avg <- mean(score) # 산술평균
diff <- (score - avg)
diff <- (score - avg)^2
diff
diff <- (score - avg)^2
diff_sum <- sum(diff)
var <- diff_sum / (length(score) - 1)
var
sd <- sqrt(var)
sd
# 1) key 생략 : [key =] value
list <- list('lee',"이순신",29,"hong","홍길동",45)
# key -> value 접근
list[[1]] -> "lee"의 키값
# key -> value 접근
list[[1]]  #-> "lee"의 key
list[[2]]  #-> "이순신"의 key
# index -> key:value 접근
list[4] # -> [[4]],  "hong"
install.packages("stringr")
library(stringr)
str <- "홍길동35이순신45"
str_extract_all(str, "[가-힣]{3}")
member = list(name = c("홍길순", "이순신"),
age = c(33,44),
gender = ㅊ("여","남"))
member
member = list(name = c("홍길순", "이순신"),
age = c(33,44),
gender = c("여","남"))
member
member$name[2]
# 조건4) 행 단위 분산과 표준편차 구하기
#  힌트 : var(), sd()
apply(Data, 1, var)
apply(Data, 1, sd)
# 1. 모집단에서 표본 추출(1,000 학생)
x <- rnorm(1000, mean = 165.2, sd = 1) # 정규분포
hist(x)
# 정규성 결정
# 귀무가설 : 정규분포와 차이가 없다.
shapiro.test(x)
# 2. 평균차이 검정 : 평균 : 165.2cm
t.test(x, mu=165.2)
# t = -0.55503, df = 999, p-value = 0.579
# t, df : 검정통계량
# p-value : 유의확률
# p-value = 0.579 >= 0.05 -> 귀무가설 채택
# 95 percent confidence interval: 95%
# 165.1071 ~ 165.2309 : 신뢰구간(채택역)
# mean of x
# 165.1832 -> 실제 평균
mean(x)
# t = -0.55503, df = 999, p-value = 0.579
# t, df : 검정통계량
# p-value : 유의확률
# p-value = 0.579 >= 0.05 -> 귀무가설 채택
# 95 percent confidence interval: 95%
# 165.1071 ~ 165.2309 : 신뢰구간(채택역)
# mean of x
# 165.169 -> 실제 평균
mean(x) # 165.169
# 3. 기각역의 평균검정
t.test(x, mu=165.09, conf.level = 0.95) # 95%
# 4. 신뢰수준 변경(95% -> 99%)
t.test(x, mu=165.2, conf.level = 0.99)
# 정규붙포
n <- 2000
rnorm(n, mean = 100, sd = 10)
x <- rnorm(n, mean = 100, sd = 10)
x
shapiro.test(x)
# 표준화
# 표준화 공식(z) = (x - mu) / sd(x)
mu <- mean(x)
# 표준화
# 표준화 공식(z) = (x - mu) / sd(x)
mu <- mean(x)
mu
z = (x - mu) / sd(x)
z
hist(z)
mean(z)
sd(z) # 1
?sd
# scale() 함수 이용
z2 <- as.data.frame(scale(x)) # matrix -> data.frame
z2
str(z2)
z2 <- z2$V1
hist(z2)
z2
mean(z2) # # 6.1
sd(z2)
# 2. 정규화
# - 특정 변수값을 일정한 범위(0 ~ 1)로 일치시키는 과정
str(iris)
head(iris)
summary(iris[-5])
# 정규화 -> data.frame 형변환
as.data.frame(scale(iris[-5]))
# 정규화 -> data.frame 형변환
iris_nor <- as.data.frame(scale(iris[-5]))
summary(iris_nor)
summary(iris[-5])
summary(iris_nor)
# 2) 정규화 함수 정의(0~1)
nor <- function(x){
n <- (x - min(x)) / (max(x) - min(x))
return (n)
}
iris_nor2 <- apply(iris[-5], 2, nor)
summary(iris_nor2)
head(iris_nor2)
df <- data.frame(no, score)
no <- 1:100 # 번호
score <- runif(100, min = 40, max = 100) # 성적
df <- data.frame(no, score)
df
nrow(df)
# 15명 학생 샘플링
sample(x = nrow(df), size = 15)
# 15명 학생 샘플링
idx <- sample(x = nrow(df), size = 15)
idx
sam <- df[idx, ]
sam
# train(70%)/test(30%) dataset
idx <- sample(x = nrow(df), size = nrow(df)*0.7)
idx
train <- df[idx, ]
test <- df[-idx, ]
dim(train)
dim(test)
# 05. iris 데이터셋을 대상으로 8:2비율로 sampling하여 train과 test 셋을 만드시오.
dim(iris)
nrow(iris)
idx <- sample(x = nrow(iris), size = nrow(iris)*0.8)
idx
dim(iris)
dim(idx)
train <- df[idx, ]
dim(train)
test <- df[-idx, ]
dim(test)
train <- iris[idx, ]
test <- iris[-idx, ]
dim(train)
dim(test)
# 50% vs 50%
idx <- sample(nrow(iris), nrow(iris)*0.5)
train <- iris[idx, ]
test <- iris[-idx, ]
dim(train)
dim(test)
head(iris)
# Sepal.Length : y(종속변수)
# Petal.Length : x(독립변수)
# model : 꽃잎 길이 -> 꽃받침 길이
model <- lm(Sepal.Length ~ Petal.Length, data = train )
pred <- model$fitted.values # 예측치(꽃받침 길이)
pred
# test ataset model
model2 <- lm(Sepal.Length ~ Petal.Length, data= test)
pred2 <- model2$fitted.values
# train_y
train_x <- train$Sepal.Length # train 정답
# test_y
test_x <- test$Sepal.Length # test 정답
# 정답 vs 예측치
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
points(test_x, pred2, col="red",pch-19)
# 정답 vs 예측치
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
points(test_x, pred2, col="red",pch-19)
points(test_x, pred2, col="red",pch=19)
col = c("blue", "red"), pch = c(18,19)
# 범례
legend("topleft", legend = c("train", "test"),
col = c("blue", "red"), pch = c(18,19))
plot(train_x,pred2)
plot(train_x,pred)
# train_y
train_x <- train$Petal.Length # train 정답
# 정답 vs 예측치
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
# test_y
test_x <- test$Petal.Length # test 정답
points(test_x, pred2, col="red",pch=19) #
plot(train_x,pred)
# 정답 vs 예측치
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
points(test_x, pred2, col="red",pch=19) #
pred
# train_y
train_x <- train$Sepal.Length # train 정답
# test_y
test_x <- test$Sepal.Length # test 정답
plot(train_x, test_x, col="blue", pch=18) # plot(x,y)
points(pred, pred2, col="red",pch=19) #
# train_y
train_x <- train$Petal.Length # train 정답
# 정답 vs 예측치
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
test_y <- test$Petal.Length
points(test_x, test_y, col="red",pch=19) #
# test_y
test_x <- test$Petal.Length # test 정답
test_y <- test$Sepal.Length
plot(train_x, pred, col="blue", pch=18) # plot(x,y)
points(test_x, test_y, col="red",pch=19) #
pred
model
# y real value vs y prediction
plot(y_true, col="blue" type = "o", pch = 18)
# y real value vs y prediction
plot(y_true, col="blue", type = "o", pch = 18)
iris_data <- iris[-5]
str(iris_data)
# 1) train/test set(70 vs 30)
idx <- sample(x = nrow(iris_data), size=nrow(iris_data)*0.7,
replace = FALSE)
idx
train <- iris_data[idx,]
test <- iris_data[-idx,]
dim(train) #  105   4(y+x)
dim(test) #  45  4(y+x)
# 2) model(train)
model <- lm(Sepal.Length~.,data=train)
# 3) 모델 평가(test)
y_pred <- predict(model, test) # y 예측치
y_pred
y_true <- test$Sepal.Length # y 정답
y_true
# 평가 : mse(평균제곱오차), cor(상관계수)
mse <- mean((y_true - y_pred)^2)
mse # 0.09911753
cor(y_true, y_pred) #  0.930735
# y real value vs y prediction
plot(y_true, col="blue", type = "o", pch = 18)
points(y_pred, col="red", type = "o", pch = 18)
#범례
legend("topleft", legend = c("real","pred"), col=c("blue","red"), pch - c(18, 19))
names(iris)
# 1. 변수 모델링 : y:Sepal.Length <- x:Sepal.Width,Petal.Length,Petal.Width
formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width
# 2. 회귀모델 생성
model <- lm(formula = formula,  data=iris)
model
names(model)
# 3. 모형의 잔차검정
plot(model, whih)
# 3. 모형의 잔차검정
plot(model)
# (1) 등분산성 검정
plot(model, which =  1)
methods('plot') # plot()에서 제공되는 객체 보기
# (2) 잔차 정규성 검정
attributes(model) # coefficients(계수), residuals(잔차), fitted.values(적합값)
res <- residuals(model) # 잔차 추출
res <- model$residuals # # 잔차추출
res
shapiro.test(res) # 정규성 검정 - p-value = 0.9349 >= 0.05
# 정규성 시각화
hist(res, freq = F)
qqnorm(res)
# (3) 잔차의 독립성(자기상관 검정 : Durbin-Watson)
install.packages('lmtest')
library(lmtest) # 자기상관 진단 패키지 설치
dwtest(model) # 더빈 왓슨 값
# 4. 다중공선성 검사
library(car)
sqrt(vif(model)) > 2 # TRUE
library(car)
sqrt(vif(model)) > 2 # TRUE
# 5. 모델 생성/평가
formula = Sepal.Length ~ Sepal.Width + Petal.Length
model <- lm(formula = formula,  data=iris)
summary(model) # 모델 평가
cor(iris[-5])
# 1) 로짓변환 : 오즈비에 log(자연로그)함수 적용
p = 0.5 # 성공확률
odds_ratio <- p / (1-p)
logit1 <- log(odds_ratio)
# 1) 로짓변환 : 오즈비에 log(자연로그)함수 적용
p = 0.5 # 성공확률
odds_ratio <- p / (1-p)
logit1 <- log(odds_ratio)
logit1
# 2) sigmoid function
sig <- 1 / (1 + exp(-logit))
# 2) sigmoid function
sig <- 1 / (1 + exp(-logit1))
sig
# 2) sigmoid function
sig1 <- 1 / (1 + exp(-logit1)) # 0
sig1 # 0.5
sig2 <- 1 / (1 + exp(-logit1)) # Inf
sig2 # 1
sig3 <- 1 / (1 + exp(-logit1)) # -Inf
sig3
# 단계1. 데이터 가져오기
weather = read.csv("C:/Rwork/data/weather.csv", stringsAsFactors = F)
dim(weather)  # 366  15
head(weather)
str(weather)
# chr 칼럼, Date, RainToday 칼럼 제거
weather_df <- weather[, c(-1, -6, -8, -14)]
str(weather_df)
#  단계3.  로지스틱  회귀모델 생성 : 학습데이터
weater_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weater_model
#  단계2.  데이터 셈플링 70 vs 30
idx <- sample(1:nrow(weather_df), nrow(weather_df)*0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
#  단계3.  로지스틱  회귀모델 생성 : 학습데이터
weater_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weater_model
# RainTomorrow 칼럼 -> 로지스틱 회귀분석 결과(0,1)에 맞게 더미변수 생성
weather_df$RainTomorrow[weather_df$RainTomorrow=='Yes'] <- 1
weather_df$RainTomorrow[weather_df$RainTomorrow=='No'] <- 0
weather_df$RainTomorrow <- as.numeric(weather_df$RainTomorrow)
head(weather_df)
#  단계2.  데이터 셈플링 70 vs 30
idx <- sample(1:nrow(weather_df), nrow(weather_df)*0.7)
train <- weather_df[idx, ]
test <- weather_df[-idx, ]
#  단계3.  로지스틱  회귀모델 생성 : 학습데이터
weater_model <- glm(RainTomorrow ~ ., data = train, family = 'binomial')
weater_model
summary(weater_model)
# 단계4. 로지스틱  회귀모델 예측치 생성 : 검정데이터
# newdata=test : 새로운 데이터 셋, type="response" : 0~1 확률값으로 예측
pred <- predict(weater_model, newdata=test, type="response")
pred
summary(pred)
str(pred)
range(pred, na.rm = T)
# cut off = 0.5 적용
ifelse(pread >= 0.5, "Yes", "No")
# 단계4. 로지스틱  회귀모델 예측치 생성 : 검정데이터
# newdata=test : 새로운 데이터 셋, type="response" : 0~1 확률값으로 예측
pred <- predict(weater_model, newdata=test, type="response")
pred
range(pred, na.rm = T) # 0.001175185  ~  0.992686899
summary(pred)
str(pred) # Named num [1:110]
# cut off = 0.5 적용
ifelse(pread >= 0.5, "Yes", "No")
# cut off = 0.5 적용
cpred <- ifelse(pred >= 0.5, "Yes", "No")
cpred
table(y_true, cpred)
y_true <- test$RainTomorrow # 정답(0, 1)
table(y_true, cpred)
# 교차분할표(confusion matrix)
t <- table(y_true, cpred)
t
acc <- (87+8) / sum(t)
acc
cat('accuracy=', acc)
acc
(4+8)/sum(t) #
acc_no <- 83/87
acc_no <- 83/87 #
acc_no
acc_yes <- 13/21 #
acc_yes
acc_no <- 83/87 # 0.954023
install.packages("ROCR")
library(ROCR)
?glm
# 재현률(민감도) : YES -> YES
recall <- 13/21 # 0.6190476
recall
# 정확률 : model(yes) -> yes
precision <- 13 / 17
# F1 Score : no != yes(불균형)
f1_score = 2*((precision * recall) / (precision + recall))
f1_score
install.packages("ROCR")
install.packages("ROCR")
library(ROCR)
# ROCR 패키지 제공 함수 : prediction() -> performance
pr <- prediction(pred, test$RainTomorrow)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
# 파일 불러오기
setwd("c:/Rwork/data")
admit <- read.csv("admit.csv")
str(admit) # 'data.frame':	400 obs. of  4 variables:
# 1. train/test data 구성
idx <- sample(x=nrow(admit), size = nrow(admit))
# 1. train/test data 구성
idx <- sample(x=nrow(admit), size = nrow(admit)*0.7)
idx
train <- admit[idx, ]
test <- admit[-idx, ]
train
test
dim(train)
dim(test)
dim(admit)
# 2. model 생성
model <- glm(admit ~ ., data = admit, family = "binomical")
# 2. model 생성
model <- glm(admit ~ ., data = admit, family = "binomial")
model
# 3. predict 생성
y_pred <- predict(model_admit, newdata = test_admin, type = "response")
# 3. predict 생성
y_pred <- predict(model, newdata = test_admin, type = "response")
# 3. predict 생성
y_pred <- predict(model, newdata = test, type = "response")
y
y_pred
y_true <- test$admit
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
y_pred <- ifelse(pred>= 0.5,1,0)
y_true <- test$admit
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
# 3. predict 생성
y_pred <- predict(model, newdata = test, type = "response")
y_pred <- ifelse(pred>= 0.5,1,0)
y_true <- test$admit
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
dim(y_true)
dim(y_pred)
dim(y_true)
dim(y_pred)
y_pred
nrow(y_true)
nrow(y_true)
nrow(y_pred)
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
install.packages("nnet")
library(nnet)
# 이항분류 vs 다항분류
# sigmoid : 이항분류(0~1확률, cutoff=0.5)
# softmax : 다항분류(0~1확률, 확률합 = 1)
# ex) class1=0.9, class2=0.1, class3=0.1
name(iris)
# 이항분류 vs 다항분류
# sigmoid : 이항분류(0~1확률, cutoff=0.5)
# softmax : 다항분류(0~1확률, 확률합 = 1)
# ex) class1=0.9, class2=0.1, class3=0.1
names(iris)
idx <- sample(nrow(iris), nrow(iris)*0.7)
train_iris <- iris[idx, ]
test_iris <- iris[-idx, ]
model <- multinom(Species ~ .,data = train_iris)
# model 평가 : test
y_pred <- predict(model, test_iris)
# model 평가 : test
y_pred <- predict(model, test_iris, type = "probs") # type = "probs"  <- 확률로 결과를 반환
y_pred
# type = "probs"  <- 확률로 결과를 반환 ()
range(y_pred)
# type = "probs"  <- 확률로 결과를 반환 (합 = 1)
y_pred <- predict(model, test_iris, type = "class")
y_pred
# type = "probs"  : 확률로 결과를 반환 (합 = 1)
str(y_pred)
y_true <- test_iris$Species
table(y_true, y_pred)
acc <-(t[1,1] + t[2,2] + t[3,3]) / sum (t)
t <- table(y_true, y_pred)
acc <-(t[1,1] + t[2,2] + t[3,3]) / sum (t)
acc
# 2. model 생성
model <- glm(admit ~ ., data = train, family = "binomial")
model
# 3. predict 생성
y_pred <- predict(model, newdata = test, type = "response")
y_pred <- ifelse(pred >= 0.5,1,0)
y_true <- test$admit
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
# 3. predict 생성
pred <- predict(model, newdata = test, type = "response")
y_pred <- ifelse(pred >= 0.5,1,0)
y_true <- test$admit
# 4. 모델 평가(분류정확도) : 혼돈 matrix 이용/ROC Curve 이용
table(y_true, y_pred)
# 1) 혼돈 matrix 이용
acc <- (72+10)/nrow(test)
acc
# 2) ROCR 패키지 제공 함수 : prediction() -> performance
library(ROCR)
# ROCR 패키지 제공 함수 : prediction() -> performance
pr <- prediction(pred, test$admit)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
